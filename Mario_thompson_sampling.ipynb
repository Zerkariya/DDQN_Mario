{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Final Project: Using Double Deep Q-Learning to play Super Mario Bro\n",
    "\n",
    "### This project references the official Pytorch's tutorial, github's code and youtube videos.\n",
    "I would list their links here:  \n",
    "  \n",
    "https://github.com/Kautenja/playing-mario-with-deep-reinforcement-learning  \n",
    "https://github.com/giorgioskij/SuperMario-RL  \n",
    "https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html  \n",
    "https://www.youtube.com/watch?v=oskrcfjVFC0  \n",
    "https://www.youtube.com/watch?v=O2QaSh4tNVw  \n",
    "https://www.youtube.com/watch?v=_gmQZToTMac  \n",
    "\n",
    "### We mainly accomplish following three stuff:\n",
    "\n",
    "1. We reproduce the paper's double deep q learning algorithm.\n",
    "2. We apply this algorithm on a specific mission which is Super Mario\n",
    "3. We extend the experiment results as changing the exploration strategy (from greedy to softmax and thompson sampling), and also change the hyperparameters to test the stability of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ueu7j2qSbmhE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random, datetime, os\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "from DealingStates import SkipFrame, GrayScaleObservation, ResizeObservation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YDCLiKwLbmhE"
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mYXW_FhVbmhF"
   },
   "source": [
    "## Environment\n",
    "\n",
    "### Initialize Environment\n",
    "\n",
    "The environment consists of tubes, mushrooms, blocks, and even components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsqVEOiEbmhF",
    "outputId": "f5709aa2-8569-4e95-8f37-102c0e7c9692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jHDdCBKRbmhG"
   },
   "source": [
    "### Dealing with states\n",
    "\n",
    "In order to understand the dynamic environment, we need to use a stack a of the most recent frames to give a sense of motion.  \n",
    "In super mario game, each state can be seen as a frame which is [3, 240, 256]. 3 is for 3 dimensions' RGB frame. 240 and 256 are for width and length.  \n",
    "\n",
    "However, the color does not matter. Hence, we can change the frame from colorful to greyscale as [1,240,256]. Further, resize the image to reduce the resources comsuption and we have [1,84,84].\n",
    "\n",
    "At lask, stacking the wrapped states, we have [4,84,84]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oaUOHrnHbmhG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarioNet import MarioNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3mENGvOmbmhI"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, exploration_rate, decay_rate, gamma, target_sync_frequency):\n",
    "\n",
    "       \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float().to(self.device)\n",
    "\n",
    "\n",
    "        # Exploration settings\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.exploration_rate_min   = 0.1\n",
    "        self.steps_taken  = 0\n",
    "\n",
    "\n",
    "\n",
    "        self.memory = TensorDictReplayBuffer(\n",
    "            storage=LazyMemmapStorage(\n",
    "                50000, \n",
    "                device=torch.device(\"cpu\"), \n",
    "                scratch_dir=\"/root/autodl-tmp/thompson_sampling2/mem/\")\n",
    "        )\n",
    "\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        self.burn_in_period = 1e4\n",
    "        self.learning_frequency = 3\n",
    "        self.target_sync_frequency = target_sync_frequency\n",
    "        \n",
    "    def act(self, state):\n",
    "        ## This is thompson sampling implementation\n",
    "        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "        state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        action_values = self.net(state, model=\"online\")\n",
    "        noise = torch.randn_like(action_values) * self.exploration_rate\n",
    "        noisy_action_values = action_values + noise\n",
    "\n",
    "        action_idx = torch.argmax(noisy_action_values, axis=1).item()\n",
    "\n",
    "        self.exploration_rate *= self.decay_rate\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        self.steps_taken += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, env_state, subsequent_state, action_taken, reward_earned, episode_done):\n",
    "\n",
    "        def process_state(state):\n",
    "            return state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "\n",
    "        processed_env_state = torch.tensor(process_state(env_state))\n",
    "        processed_subsequent_state = torch.tensor(process_state(subsequent_state))\n",
    "        tensor_action = torch.tensor([action_taken])\n",
    "        tensor_reward = torch.tensor([reward_earned])\n",
    "        tensor_done = torch.tensor([episode_done])\n",
    "\n",
    "        experience_data = {\n",
    "            \"state\": processed_env_state, \n",
    "            \"next_state\": processed_subsequent_state, \n",
    "            \"action\": tensor_action, \n",
    "            \"reward\": tensor_reward, \n",
    "            \"done\": tensor_done\n",
    "        }\n",
    "\n",
    "        self.memory.add(TensorDict(experience_data, batch_size=[]))\n",
    "\n",
    "\n",
    "    \n",
    "    def recall(self):\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        extracted_states = batch.get(\"state\")\n",
    "        extracted_next_states = batch.get(\"next_state\")\n",
    "        extracted_actions = batch.get(\"action\").squeeze()\n",
    "        extracted_rewards = batch.get(\"reward\").squeeze()\n",
    "        extracted_dones = batch.get(\"done\").squeeze()\n",
    "\n",
    "        return extracted_states, extracted_next_states, extracted_actions, extracted_rewards, extracted_dones\n",
    "\n",
    "\n",
    "    def td_values(self, state, action, reward, next_state, done):\n",
    "        td_estimate = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            next_state_Q = self.net(next_state, model=\"online\")\n",
    "            best_action = torch.argmax(next_state_Q, axis=1)\n",
    "            next_Q = self.net(next_state, model=\"target\")[\n",
    "                np.arange(0, self.batch_size), best_action\n",
    "            ]\n",
    "            td_target = (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "        return td_estimate, td_target\n",
    "\n",
    "\n",
    "    \n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "    \n",
    "    def learn(self):\n",
    "       \n",
    "        if self.steps_taken % self.target_sync_frequency == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.steps_taken % 50000 == 0:\n",
    "            self.save_model_checkpoint()\n",
    "\n",
    "        if self.steps_taken < self.burn_in_period or self.steps_taken % self.learning_frequency != 0:\n",
    "            return None, None\n",
    "\n",
    "        fetched_states, fetched_next_states, fetched_actions, fetched_rewards, fetched_dones = self.recall()\n",
    "        td_estimate, td_target = self.td_values(fetched_states, fetched_actions, fetched_rewards, fetched_next_states, fetched_dones)\n",
    "\n",
    "        training_loss = self.update_Q_online(td_estimate, td_target)\n",
    "        return td_estimate.mean().item(), training_loss\n",
    "\n",
    "    def save_model_checkpoint(self):\n",
    "\n",
    "        checkpoint_path = self.save_dir / f\"mario_net_{int(self.steps_taken // 50000)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            checkpoint_path\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pqMZhv58bmhL"
   },
   "source": [
    "### Logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "icgv4sY1bmhL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        self._initialize_log_file()\n",
    "\n",
    "        # Dictionary to hold history metrics\n",
    "        self.metrics = {\n",
    "            'rewards': [],\n",
    "            'lengths': [],\n",
    "            'losses': [],\n",
    "            'q_values': [],\n",
    "            'avg_rewards': [],\n",
    "            'avg_lengths': [],\n",
    "            'avg_losses': [],\n",
    "            'avg_q_values': []\n",
    "        }\n",
    "\n",
    "        # Current episode metrics\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def _initialize_log_file(self):\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            headers = (\n",
    "                \"Episode\", \"Step\", \"Epsilon\", \"MeanReward\",\n",
    "                \"MeanLength\", \"MeanLoss\", \"MeanQValue\", \"TimeDelta\", \"Time\"\n",
    "            )\n",
    "            f.write(f\"{headers[0]:>8}{headers[1]:>8}{headers[2]:>10}{headers[3]:>15}\"\n",
    "                    f\"{headers[4]:>15}{headers[5]:>15}{headers[6]:>15}{headers[7]:>15}{headers[8]:>20}\\n\")\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.metrics['rewards'].append(self.curr_ep_reward)\n",
    "        self.metrics['lengths'].append(self.curr_ep_length)\n",
    "        avg_loss = avg_q = 0\n",
    "        if self.curr_ep_loss_length > 0:\n",
    "            avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.metrics['losses'].append(avg_loss)\n",
    "        self.metrics['q_values'].append(avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_reward = np.round(np.mean(self.metrics['rewards'][-100:]), 3)\n",
    "        mean_length = np.round(np.mean(self.metrics['lengths'][-100:]), 3)\n",
    "        mean_loss = np.round(np.mean(self.metrics['losses'][-100:]), 3)\n",
    "        mean_q = np.round(np.mean(self.metrics['q_values'][-100:]), 3)\n",
    "\n",
    "        self.metrics['avg_rewards'].append(mean_reward)\n",
    "        self.metrics['avg_lengths'].append(mean_length)\n",
    "        self.metrics['avg_losses'].append(mean_loss)\n",
    "        self.metrics['avg_q_values'].append(mean_q)\n",
    "\n",
    "        time_delta = np.round(time.time() - self.record_time, 3)\n",
    "        self.record_time = time.time()\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_reward} - \"\n",
    "            f\"Mean Length {mean_length} - \"\n",
    "            f\"Mean Loss {mean_loss} - \"\n",
    "            f\"Mean Q Value {mean_q} - \"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_reward:15.3f}{mean_length:15.3f}{mean_loss:15.3f}{mean_q:15.3f}\"\n",
    "                f\"{time_delta:15.3f}{self.record_time:20.3f}\\n\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruxXWbocbmhM",
    "outputId": "bc8c84d3-b12d-4591-d1a3-4f26e5dc3bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 107 - Epsilon 0.9999973250035489 - Mean Reward 628.0 - Mean Length 107.0 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 20 - Step 4822 - Epsilon 0.99987945726455 - Mean Reward 607.333 - Mean Length 229.619 - Mean Loss 0.0 - Mean Q Value 0.0 - \n",
      "Episode 40 - Step 10468 - Epsilon 0.9997383342376162 - Mean Reward 652.024 - Mean Length 255.317 - Mean Loss 0.124 - Mean Q Value 0.034 - \n",
      "Episode 60 - Step 13474 - Epsilon 0.9996632067239334 - Mean Reward 621.541 - Mean Length 220.885 - Mean Loss 0.318 - Mean Q Value 0.86 - \n",
      "Episode 80 - Step 15842 - Epsilon 0.9996040284131473 - Mean Reward 599.852 - Mean Length 195.58 - Mean Loss 0.347 - Mean Q Value 1.384 - \n",
      "Episode 100 - Step 20461 - Epsilon 0.9994886058010536 - Mean Reward 601.61 - Mean Length 203.54 - Mean Loss 0.364 - Mean Q Value 1.751 - \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling copy 3.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m next_state, reward, done, trunc, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m mario\u001b[39m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m q, loss \u001b[39m=\u001b[39m mario\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m logger\u001b[39m.\u001b[39mlog_step(reward, loss, q)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "\u001b[1;32m/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling copy 3.ipynb Cell 15\u001b[0m in \u001b[0;36mMario.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m fetched_states, fetched_next_states, fetched_actions, fetched_rewards, fetched_dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecall()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m td_estimate, td_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtd_values(fetched_states, fetched_actions, fetched_rewards, fetched_next_states, fetched_dones)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_Q_online(td_estimate, td_target)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39mreturn\u001b[39;00m td_estimate\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem(), training_loss\n",
      "\u001b[1;32m/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling copy 3.ipynb Cell 15\u001b[0m in \u001b[0;36mMario.update_Q_online\u001b[0;34m(self, td_estimate, td_target)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(td_estimate, td_target)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/autodl-tmp/my/Copy_of_mario_rl_tutorial_thompson_sampling%20copy%203.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = Path(\"/root/autodl-tmp/thompson_sampling2/checkpoints/\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "\n",
    "exploration_rate = 1\n",
    "decay_rate = 0.999999975\n",
    "gamma = 0.1\n",
    "target_sync_frequency = 1e4\n",
    "\n",
    "mario = Mario(\n",
    "    state_dim=(4, 84, 84), \n",
    "    action_dim=env.action_space.n, \n",
    "    save_dir=save_dir, \n",
    "    exploration_rate = 1,\n",
    "    decay_rate = 0.999999975,\n",
    "    gamma = 0.1,\n",
    "    target_sync_frequency = 1e4)\n",
    "\n",
    "\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 100000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        action = mario.act(state)\n",
    "\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.steps_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
